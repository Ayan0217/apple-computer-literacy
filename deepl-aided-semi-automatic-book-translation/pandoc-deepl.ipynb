{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d99812f-b974-4af3-991b-b6a8f2d83da4",
   "metadata": {},
   "source": [
    "## pandoc + DeepL\n",
    "\n",
    "### 1. 使用 Homebrew 在 MacOS 上安装 pandoc：\n",
    "\n",
    "```bash\n",
    "brew install pandoc\n",
    "```\n",
    "\n",
    "### 2. 获取 pandoc 的 path 用来为 pypandoc 设置路径：\n",
    "\n",
    "```bash\n",
    "which pandoc\n",
    "\n",
    "$ /opt/homebrew/bin/pandoc\n",
    "```\n",
    "\n",
    "### 3. 使用 pypandoc 转换 epub 为 html："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff08b8d5-00c2-4028-a266-68716fe1a66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install progressbar2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a99343-5065-4d32-9863-d27f5cb87470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypandoc\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "os.environ.setdefault('PYPANDOC_PANDOC', '/opt/homebrew/bin/pandoc')\n",
    "\n",
    "\n",
    "filepath = \"/Users/joker/Public/bourgeois/Bourgeios Dignity/Bourgeios Dignity.epub\"\n",
    "pathname, filename = os.path.split(filepath)\n",
    "targetfilename = 'index.html'\n",
    "\n",
    "\n",
    "pypandoc.convert_file(filepath,\n",
    "                      format='epub',\n",
    "                      to='html5',\n",
    "                      extra_args=[\n",
    "                          '--read=epub',\n",
    "                          f'--extract-media={pathname}/images',\n",
    "                          '--wrap=none',\n",
    "                          '--standalone'\n",
    "                      ],\n",
    "                      encoding='utf-8',\n",
    "                      outputfile=pathname + '/' + targetfilename,\n",
    "                      filters=None,\n",
    "                      verify_format=True\n",
    "                     )\n",
    "\n",
    "os.system(f\"open '{pathname}'\")\n",
    "\n",
    "# brew install vscode\n",
    "# which code -> /opt/homebrew/bin/code\n",
    "# os.system(f\"/opt/homebrew/bin/code '{pathname}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4948ae6e-79ff-4ffa-bc3c-2ed9eb17d132",
   "metadata": {},
   "source": [
    "### 4. 为 pandoc 转换的 html 添加 header、footer 以及 css："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb554296-d4d7-4d0c-969a-5eb298261368",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "\n",
    "# filepath = \"/Users/joker/Calibre Library/craft of research/The Craft of Research by Wayne C. Booth, Gregory G. Colomb et al. (z-lib.org).epub\"\n",
    "# pathname, filename = os.path.split(filepath)\n",
    "# targetfilename = 'index-en-zhcn.html'\n",
    "\n",
    "title = Path(filepath).stem\n",
    "\n",
    "htmlheader = f\"\"\"\n",
    "<html>\n",
    "  <head>\n",
    "    <meta http-equiv=\"Content-Type\" content=\"text/html;charset=utf-8\" />\n",
    "    <link href=\"./style.css\" rel=\"stylesheet\" type=\"text/css\" />\n",
    "    <title>{title}</title>\n",
    "  </head>  \n",
    "<body>\n",
    "\"\"\"\n",
    "\n",
    "htmlfooter = \"\"\"\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "stylecss = \"\"\"\n",
    "body {\n",
    "  width: 90%;\n",
    "  margin: 1em auto;\n",
    "  font-family:\"Kaiti SC\", Georgia, 'Times New Roman', Times, serif !important;\n",
    "  font-size: 20px;\n",
    "  color: #35453F;\n",
    "  background-color: #C4E7CD;\n",
    "}\n",
    "\n",
    "img{\n",
    "  /* width: 90% !important; */\n",
    "  text-align: center !important;\n",
    "  border: #35453F solid 10 px;\n",
    "  padding: 10px;\n",
    "}\n",
    "\n",
    "img.inline {\n",
    "  height: 1em;\n",
    "  width:auto !important;\n",
    "  margin-bottom: -0.6em !important;\n",
    "}\n",
    "\n",
    "p {\n",
    "  margin-bottom: 1em !important;\n",
    "  font-size: 20px !important;\n",
    "}\n",
    "\n",
    "h1.english, h2.english, p.english {\n",
    "  /* display: none; */\n",
    "}\n",
    "\n",
    "h1.chinese, h2.chinese, p.chinese {\n",
    "  letter-spacing: 0.1em;\n",
    "}\n",
    "\n",
    "sup {\n",
    "  margin-right: 0.5em;\n",
    "}\n",
    "\n",
    "a {\n",
    "  color: #232442\n",
    "}\n",
    "\n",
    "strong {\n",
    "  color: #151741; \n",
    "  font-size: 95%;\n",
    "}\n",
    "\n",
    ".chinese em, .chinese i {\n",
    "  font-style: normal;\n",
    "  font-weight: bold;\n",
    "  color: #151741; \n",
    "  font-size: 95%;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "with open(pathname + '/' + targetfilename, \"r\") as file:\n",
    "    html = file.read()\n",
    "\n",
    "# pypandoc 在转换时，会将 pathname 写入 html 文件，\n",
    "# 以下 regex 是将 img tag 中的 pathname 替换掉\n",
    "pttn = rf'{pathname}'\n",
    "rpl = r'.'\n",
    "html = re.sub(pttn, rpl, html)    \n",
    "    \n",
    "with open(pathname + '/' + targetfilename, \"w\") as file:\n",
    "    file.write(htmlheader)\n",
    "    file.write(html)\n",
    "    file.write(htmlfooter)\n",
    "    \n",
    "with open(pathname + '/style.css', \"w\") as file:\n",
    "    file.write(stylecss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759085fd-909c-4201-a769-a7f34a627f21",
   "metadata": {},
   "source": [
    "### 5. 清理 html\n",
    "\n",
    "提交 DeepL 翻译之前，有必要大致浏览一下 html 文件。\n",
    "\n",
    "1. 看看有没有什么需要清理的地方（比如，多余的 span tag）\n",
    "2. 看看需要翻译的都包括哪些 tags？（在下一个代码块中需要指定）\n",
    "\n",
    "如有必要清理，可使用以下脚本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecd5546-ec86-4ab3-aa35-f462c2424304",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_filename = pathname + '/' + targetfilename\n",
    "                        \n",
    "target_filename = pathname + '/' + targetfilename\n",
    "\n",
    "def remove_span_with_no_attribute(html):\n",
    "    soup = BeautifulSoup(html)\n",
    "    for span in soup.find_all('span', attrs={}):\n",
    "        if span.attrs == {}:\n",
    "            span.unwrap()    \n",
    "    return str(soup)\n",
    "\n",
    "def remove_span_with_class(html, classes):\n",
    "    soup = BeautifulSoup(html)\n",
    "    for span in soup.find_all('span', attrs={'class':classes}):\n",
    "        span.unwrap()    \n",
    "    return str(soup)\n",
    "\n",
    "def remove_br(html):\n",
    "    pttn = r'<br\\s*/>\\n+'\n",
    "    rpl = r''\n",
    "    html = re.sub(pttn, rpl, html)\n",
    "    return html\n",
    "\n",
    "def remove_tags(html, tags):\n",
    "    soup = BeautifulSoup(html)\n",
    "    for tag in tags:\n",
    "        for each_tag in soup.find_all(tag):\n",
    "            each_tag.unwrap()\n",
    "    return str(soup)\n",
    "\n",
    "def remove_empty_tags(html):\n",
    "    soup = BeautifulSoup(html)\n",
    "    for x in soup.find_all():\n",
    "        if len(x.get_text(strip=True)) == 0:\n",
    "            x.extract()\n",
    "    return str(soup)\n",
    "\n",
    "def remove_empty_lines(html):\n",
    "    pttn = r'\\n\\s*\\n'\n",
    "    rpl = r'\\n'\n",
    "    html = re.sub(pttn, rpl, html)\n",
    "    return html\n",
    "\n",
    "def remove_all_spans(html):\n",
    "    soup = BeautifulSoup(html)\n",
    "    for span in soup.find_all('span'):\n",
    "        span.unwrap()    \n",
    "    return str(soup)\n",
    "\n",
    "\n",
    "def change_tag_from_to(html, original, new):\n",
    "    soup = BeautifulSoup(html)\n",
    "    for tag in soup.find_all(original):\n",
    "        tag.content = tag.get_text().strip()\n",
    "        tag.name = new      \n",
    "    return str(soup)    \n",
    "\n",
    "def change_tag_with_class(html, tagfrom, tagto, classfrom, classto):\n",
    "    soup = BeautifulSoup(html)\n",
    "    for tag in soup.find_all(tagfrom, attrs={'class':classfrom}):\n",
    "        tag.name = tagto\n",
    "        tag.attrs = {'class':classto}\n",
    "        # tag.contents.remove('\\n')\n",
    "        # tag.contents.remove('\\n')\n",
    "    return str(soup)\n",
    "\n",
    "\n",
    "\n",
    "with open(source_filename, \"r\") as file:\n",
    "    html = file.read()\n",
    "\n",
    "# html = remove_span_with_no_attribute(html)\n",
    "# html = remove_span_with_class(html, ['small', 'Dropcap', 'pagebreak', 'smallCaps', 'bold', 'italic', 'calibre3', 'calibre4', 'calibre5'])\n",
    "# html = remove_all_spans(html)\n",
    "# html = remove_br(html)\n",
    "# html = remove_tags(html, ['big', 'div', 'aside', 'section', 'span'])\n",
    "# html = remove_empty_tags(html)\n",
    "html = remove_empty_lines(html)\n",
    "# html = change_tag_from_to(html, 'blockquote', 'p')\n",
    "\n",
    "html = change_tag_with_class(html, \"div\", \"blockquote\", \"fmtx1\", \"fmtx1\")\n",
    "# html = change_tag_with_class(html, \"div\", \"p\", \"fmtx\", \"fmtx\")\n",
    "# html = change_tag_with_class(html, \"div\", \"p\", \"ctag1\", \"ctag1\")\n",
    "# html = change_tag_with_class(html, \"div\", \"p\", \"crt\", \"crt\")\n",
    "# html = change_tag_with_class(html, \"div\", \"p\", \"tx\", \"tx\")\n",
    "# html = change_tag_with_class(html, \"div\", \"p\", \"tx1\", \"tx1\")\n",
    "html = change_tag_with_class(html, \"div\", \"li\", \"cct\", \"cct\")\n",
    "html = change_tag_with_class(html, \"div\", \"li\", \"apt\", \"apt\")\n",
    "html = change_tag_with_class(html, \"div\", \"li\", \"cbmh\", \"cbmh\")\n",
    "html = change_tag_from_to(html, \"div\", \"p\")\n",
    "# html = change_tag_with_class(html, \"div\", \"p\", \"class_s2G\", \"class_s2G\")\n",
    "# html = change_tag_with_class(html, \"div\", \"li\", \"class_s13\", \"class_s13\")\n",
    "# html = change_tag_with_class(html, \"div\", \"ul\", \"class_s1S\", \"class_s1S\")\n",
    "# html = change_tag_with_class(html, \"div\", \"blockquote\", \"class_s22\", \"class_s22\")\n",
    "# html = change_tag_with_class(html, \"div\", \"blockquote\", \"class_s20\", \"class_s20\")\n",
    "\n",
    "\n",
    "with open (target_filename, \"w\") as file:\n",
    "    file.write(html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7613afe9-117d-40cf-9f0a-86a3c56772d3",
   "metadata": {},
   "source": [
    "### 6. 提交 DeepL 翻译\n",
    "\n",
    "以下脚本中，line 170-189，需要在执行前指定一些参数，参见以下代码中的备注部分：\n",
    "\n",
    "```python\n",
    "# tags_to_be_translated = ['p', 'h1', 'h2', 'h3', 'h4']\n",
    "tags_to_be_translated = ['p', 'li', 'td', 'h1', 'h2', 'h3', 'h4']\n",
    "tags_tbt = '|'.join(tags_to_be_translated) # 为了以后在 regular expression 中使用\n",
    "\n",
    "# pathname  = \"/Users/joker/Public/The future of everything/\" # 文件夹名称末尾得有 / \n",
    "\n",
    "source_filename = \"index.html\"  # 用 pandoc 转换生成的文件，成为这一步的 “源文件”\n",
    "                        \n",
    "target_filename = \"index-en-zhcn.html\"\n",
    "\n",
    "lines = open(pathname + '/' + source_filename, \"r\").readlines()\n",
    "\n",
    "new_lines = []\n",
    "line_count = 0\n",
    "# 指定从哪一行开始翻译\n",
    "startline = 313\n",
    "# 指定到哪一行停止翻译\n",
    "endline = 4746\n",
    "# 是否是重新尝试\n",
    "retry = 0source_filename = \"index.html\"  # 用 pandoc 转换生成的文件，成为这一步的 “源文件”\n",
    "                        \n",
    "target_filename = \"index-en-zhcn.html\"\n",
    "\n",
    "lines = open(pathname + '/' + source_filename, \"r\").readlines()\n",
    "\n",
    "new_lines = []\n",
    "line_count = 0\n",
    "# 指定从哪一行开始翻译\n",
    "startline = 313\n",
    "# 指定到哪一行停止翻译\n",
    "endline = 4746\n",
    "# 是否是重新尝试\n",
    "retry = 0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c5dc85-dff4-44a0-9ef8-9503ed8903d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import progressbar\n",
    "\n",
    "# 若干需要使用的函数\n",
    "\n",
    "def translate(text):\n",
    "    result = requests.get( \n",
    "       \"https://api.deepl.com/v2/translate\",\n",
    "       params={ \n",
    "         \"auth_key\": auth_key,\n",
    "         \"target_lang\": target_language,\n",
    "         \"text\": text,\n",
    "         \"tag_handling\": \"xml\", # 这个参数确保 DeepL 正确处理 html tags\n",
    "       },\n",
    "    ) \n",
    "    return result.json()[\"translations\"][0][\"text\"]\n",
    "\n",
    "def add_language_tag(html, tag, classname):\n",
    "    soup = BeautifulSoup(html)\n",
    "    for the_tag in soup.find_all(tag):\n",
    "        the_tag['class'] = the_tag.get('class', []) + [classname]\n",
    "    return str(soup)\n",
    "\n",
    "def write_into_file(filename, text):\n",
    "    with open(filename, 'a', encoding='utf-8') as f:\n",
    "        f.write(\"\\n\"+text)    \n",
    "\n",
    "def zh_format(html):\n",
    "    \n",
    "    # 直双引号转换成弯双引号\n",
    "    pttn = r'\\s*\"(.*?)\\s*\"'\n",
    "    rpl = r'“\\1”'\n",
    "    html = re.sub(pttn, rpl, html)\n",
    "    \n",
    "    # 直单引号转换成弯单引号\n",
    "    pttn = r\"\\s*'(.*?)\\s*'\"\n",
    "    rpl = r'‘\\1’'\n",
    "    html = re.sub(pttn, rpl, html)\n",
    "    \n",
    "    # html tag 中被误伤的双直引号\n",
    "    pttn = r'=[“”\"](.*?)[“”\"]'\n",
    "    rpl = r'=\"\\1\"'\n",
    "    html = re.sub(pttn, rpl, html)   \n",
    "    \n",
    "    # html tag 中被误伤的单直引号\n",
    "    pttn = r\"=[‘’'](.*?)[‘’']\"\n",
    "    rpl = r\"='\\1'\"\n",
    "    html = re.sub(pttn, rpl, html)\n",
    "    \n",
    "    # 弯引号之前的空格\n",
    "    pttn = r'([\\u4e00-\\u9fa5])([“‘])'\n",
    "    rpl = r'\\1 \\2'\n",
    "    html = re.sub(pttn, rpl, html)\n",
    "\n",
    "    # 弯引号之后的空格 —— 标点符号不在 \\u4e00-\\u9fa5 范围内\n",
    "    pttn = r'([’”])([\\u4e00-\\u9fa5])'\n",
    "    rpl = r'\\1 \\2'\n",
    "    html = re.sub(pttn, rpl, html)\n",
    "           \n",
    "    # html tag: <i>, <em> 转换成 <strong>\n",
    "    pttn = r'(<i|<em)'\n",
    "    rpl = r'<strong'\n",
    "    html = re.sub(pttn, rpl, html)\n",
    "    \n",
    "    # html tag: <i>, <em> 转换成 <strong>\n",
    "    pttn = r'(/i>|/em>)'\n",
    "    rpl = r'/strong>'\n",
    "    html = re.sub(pttn, rpl, html)\n",
    "    \n",
    "    # html tag: strong 内部的 “”、‘’、《》、（）\n",
    "    pttn = r'<strong (.*?)>([《（“‘]+)'\n",
    "    rpl = r'\\2<strong \\1>'\n",
    "    html = re.sub(pttn, rpl, html)\n",
    "    \n",
    "    pttn = r'([》）”’。，]+)</strong>'\n",
    "    rpl = r'</strong>\\1'\n",
    "    html = re.sub(pttn, rpl, html)\n",
    "    \n",
    "    # 省略号\n",
    "    pttn = r'(\\. )+\\s*。*\\s*|。\\s*(\\. )+'\n",
    "    rpl = r'…… '\n",
    "    html = re.sub(pttn, rpl, html)   \n",
    "\n",
    "    # 破折号\n",
    "    pttn = r'&mdash；|&mdash;|--'\n",
    "    rpl = r' —— '\n",
    "    html = re.sub(pttn, rpl, html)\n",
    "    \n",
    "    # 姓名之间的 ·（重复三次）\n",
    "    pttn = r'([\\u4e00-\\u9fa5])-([\\u4e00-\\u9fa5])'\n",
    "    rpl = r'\\1·\\2'\n",
    "    html = re.sub(pttn, rpl, html)\n",
    "    \n",
    "    pttn = r'([\\u4e00-\\u9fa5])-([\\u4e00-\\u9fa5])'\n",
    "    rpl = r'\\1·\\2'\n",
    "    html = re.sub(pttn, rpl, html)\n",
    "    \n",
    "    pttn = r'([\\u4e00-\\u9fa5])-([\\u4e00-\\u9fa5])'\n",
    "    rpl = r'\\1·\\2'\n",
    "    html = re.sub(pttn, rpl, html)\n",
    "\n",
    "    # 姓名之间的 ·（中间含有一个英文字母的）\n",
    "    pttn = r'([\\u4e00-\\u9fa5])-(.?)-([\\u4e00-\\u9fa5])'\n",
    "    rpl = r'\\1·\\2·\\3'\n",
    "    html = re.sub(pttn, rpl, html)\n",
    "\n",
    "    # 全角百分号\n",
    "    pttn = r'％'\n",
    "    rpl = r'%'\n",
    "    html = re.sub(pttn, rpl, html)\n",
    "      \n",
    "    # 数字前的空格\n",
    "    pttn = r'([\\u4e00-\\u9fa5])(\\d)'\n",
    "    rpl = r'\\1 \\2'\n",
    "    html = re.sub(pttn, rpl, html)\n",
    "    \n",
    "    # 数字后的空格，百分比 % 后的空格\n",
    "    pttn = r'([\\d%])([\\u4e00-\\u9fa5])'\n",
    "    rpl = r'\\1 \\2'\n",
    "    html = re.sub(pttn, rpl, html)\n",
    "        \n",
    "    # 英文字母前的空格\n",
    "    pttn = r'([\\u4e00-\\u9fa5])([a-zA-Z])'\n",
    "    rpl = r'\\1 \\2'\n",
    "    html = re.sub(pttn, rpl, html)\n",
    "        \n",
    "    # 英文字母后的空格，百分比 % 后的空格\n",
    "    pttn = r'([a-zA-Z])([\\u4e00-\\u9fa5])'\n",
    "    rpl = r'\\1 \\2'\n",
    "    html = re.sub(pttn, rpl, html)\n",
    "        \n",
    "    # 弯引号前的逗号\n",
    "    pttn = r'，([”’])'\n",
    "    rpl = r'\\1，'\n",
    "    html = re.sub(pttn, rpl, html)\n",
    "        \n",
    "    # 中文标点符号之前多余的空格\n",
    "    pttn = r'([，。！？》〉】]) '\n",
    "    rpl = r'\\1'\n",
    "    html = re.sub(pttn, rpl, html)\n",
    "    \n",
    "    # 英文句号 . 与汉字之间的空格\n",
    "    pttn = r'\\.([\\u4e00-\\u9fa5])'\n",
    "    rpl = r'. \\1'\n",
    "    html = re.sub(pttn, rpl, html)\n",
    "      \n",
    "    # 左半角括号\n",
    "    pttn = r'\\s*\\('\n",
    "    rpl = r'（'\n",
    "    html = re.sub(pttn, rpl, html)\n",
    "    \n",
    "    # 右半角括号\n",
    "    pttn = r'\\)\\s*'\n",
    "    rpl = r'）'\n",
    "    html = re.sub(pttn, rpl, html)  \n",
    "\n",
    "    # 多余的括号（DeepL 返回文本经常出现的情况）\n",
    "    pttn = r'）。）'\n",
    "    rpl = r'。）'\n",
    "    html = re.sub(pttn, rpl, html)\n",
    "    \n",
    "    # 省略号\n",
    "    \n",
    "    pttn = r'(\\.\\s*){3,}\\s*'\n",
    "    rpl = r'… '\n",
    "    html = re.sub(pttn, rpl, html)\n",
    "    \n",
    "    \n",
    "    return html\n",
    "\n",
    "# 指定一些变量\n",
    "\n",
    "auth_key = \"e1b7d41f-338b-0aaf-fd1d-77d9c33262a2\" # 注意，要订阅的是 DeepL API Pro\n",
    "target_language = \"ZH\"  ## 当然，你可以将目标语言设置成任何 DeepL 支持的语言\n",
    "\n",
    "# tags_to_be_translated = ['p', 'h1', 'h2', 'h3', 'h4']\n",
    "tags_to_be_translated = ['p', 'li', 'blockquote', 'h1', 'h2', 'h3', 'h4']\n",
    "tags_tbt = '|'.join(tags_to_be_translated) # 为了以后在 regular expression 中使用\n",
    "\n",
    "# pathname  = \"/Users/joker/Public/The Tibetan Book of the Dead/\" # 文件夹名称末尾得有 /\n",
    "\n",
    "source_filename = \"index.html\"  # 用 pandoc 转换生成的文件，成为这一步的 “源文件”\n",
    "                        \n",
    "target_filename = \"index-en-zhcn.html\"\n",
    "\n",
    "lines = open(pathname + '/' + source_filename, \"r\").readlines()\n",
    "\n",
    "new_lines = []\n",
    "line_count = 0\n",
    "# 指定从哪一行开始翻译\n",
    "startline = 217\n",
    "# 指定到哪一行停止翻译\n",
    "endline = 2948\n",
    "# 是否是重新尝试\n",
    "retry = 0\n",
    "\n",
    "# 开始逐行处理\n",
    "\n",
    "with progressbar.ProgressBar(max_value=len(lines)) as bar:\n",
    "\n",
    "    for line in lines:\n",
    "\n",
    "        line_count += 1\n",
    "        # print(line_count)\n",
    "        bar.update(line_count)\n",
    "\n",
    "        if (line_count < startline) or (line_count > endline):\n",
    "            new_lines.append(line)\n",
    "            # print(line)\n",
    "            if not retry:\n",
    "                write_into_file(pathname + '/' + target_filename, line)\n",
    "            continue  \n",
    "\n",
    "        if line.strip() == '':\n",
    "            new_lines.append(line)  \n",
    "\n",
    "        tags = [tag.name for tag in BeautifulSoup(line).find_all()]\n",
    "\n",
    "        if len(tags) > 0 and line_count > startline and not retry:\n",
    "\n",
    "            to_tranlate = False\n",
    "            translating_tag = \"\"\n",
    "\n",
    "            for tag in tags:\n",
    "                if tag in tags_to_be_translated:\n",
    "                    to_tranlate = True\n",
    "                    translating_tag = tag\n",
    "\n",
    "            if to_tranlate:\n",
    "\n",
    "                succeeded = False\n",
    "                while not succeeded:\n",
    "\n",
    "                    # 以下比较粗暴的 try... except，用来防止执行过程中出现 DeepL 连接错误而导致翻译任务中断……\n",
    "\n",
    "                    soupline = BeautifulSoup(line, 'html.parser')\n",
    "\n",
    "                    line = str(soupline)\n",
    "\n",
    "                    try:\n",
    "                        line_translated = translate(line)\n",
    "\n",
    "                        # 以下一行确保将返回的字符串转换成一整行，而非含有 \\n 的多行文本\n",
    "                        line_translated = line_translated.replace(\"\\n\", \"\")\n",
    "                        succeeded = True\n",
    "                    except:\n",
    "                        succeeded = False\n",
    "\n",
    "                line = add_language_tag(line, translating_tag, 'english')\n",
    "                line_translated = add_language_tag(zh_format(line_translated), translating_tag, 'chinese')\n",
    "\n",
    "                new_lines.append(line)\n",
    "                # print(line)\n",
    "                write_into_file(pathname + '/' + target_filename, line)      \n",
    "\n",
    "                new_lines.append(line_translated)\n",
    "                # print(line_translated)\n",
    "                write_into_file(pathname + '/' + target_filename, line_translated + '\\n')\n",
    "\n",
    "            else:\n",
    "                new_lines.append(line)\n",
    "                # print(line)\n",
    "                write_into_file(pathname + '/' + target_filename, line)\n",
    "                continue\n",
    "\n",
    "            \n",
    "print ('finished!')            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef683b2-2d0b-4e54-bace-f63b7499388d",
   "metadata": {},
   "source": [
    "### 7. 对已翻译段落做批处理\n",
    "\n",
    "有时需要对已经 “中英交错排版的 html” 中的 “中文段落” 进行批处理。可用以下脚本。\n",
    "\n",
    "因为已翻译段落，被加上了 `class=\"chinese\"`，所以可以用它作为判断条件（`if '=\"chinese\"' in line:`）。\n",
    "\n",
    "if 块中的正则表达式可按需求修改："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a93120-2933-4924-9d04-f5b50bdc4b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "source_filename = \"index-en-zhcn.html\"\n",
    "                        \n",
    "target_filename = \"index-en-zhcn.html\"\n",
    "\n",
    "\n",
    "with open(pathname + '/' + source_filename, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "    \n",
    "with open(pathname + '/' + target_filename, \"w\") as file:  \n",
    "    \n",
    "    for line in lines:\n",
    "    \n",
    "        if '=\"chinese\"' in line:\n",
    "            \n",
    "            # html tag: <i>, <em> 转换成 <strong>\n",
    "            pttn = r'(<i|<em)'\n",
    "            rpl = r'<strong'\n",
    "            line = re.sub(pttn, rpl, line)\n",
    "\n",
    "            # html tag: <i>, <em> 转换成 <strong>\n",
    "            pttn = r'(/i>|/em>)'\n",
    "            rpl = r'/strong>'\n",
    "            line = re.sub(pttn, rpl, line)\n",
    "        \n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e60467-a82d-4cf5-bfdb-1f74e0e49d7a",
   "metadata": {},
   "source": [
    "### 8. 转换 html 为 epub\n",
    "\n",
    "使用 Calibre 更佳\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb26cda-ab82-461e-87ec-3e9c38923b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_span_with_class(html, classes):\n",
    "    soup = BeautifulSoup(html)\n",
    "    for span in soup.find_all('span', attrs={'class':classes}):\n",
    "        span.unwrap()    \n",
    "    return str(soup)\n",
    "\n",
    "html = \"\"\"\n",
    "<p><span class=\"Dropcap\">T</span>he world can be a <span class=\"small\">confusing</span> place if you don’t know what confirmation bias is and how often it occurs in our daily lives. Confirmation bias is the human reflex to interpret any new information as being supportive of the opinions we already hold. And it doesn’t matter how poorly the new information fits our existing views. We will twist our minds into pretzels to make the new information feel as if it is consistent with what we “know” to be true.</p>\n",
    "\"\"\"\n",
    "\n",
    "remove_span_with_class(html, ['small', 'Dropcap'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f51987-83de-4392-b19f-1da6997bee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change tag with class\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_object = \"\"\"\n",
    "<div class=\"class_s13\">\n",
    "<a href=\"#part0020.xhtml\" class=\"class_s6VF\">Appendices</a>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "def change_tag_with_class(html, tagfrom, tagto, classfrom, classto):\n",
    "    soup = BeautifulSoup(html)\n",
    "    for tag in soup.find_all(tagfrom, attrs={'class':classfrom}):\n",
    "        tag.name = tagto\n",
    "        tag.attrs = {'class':classto}    \n",
    "        tag.contents.remove('\\n')\n",
    "        tag.contents.remove('\\n')\n",
    "    return str(soup)\n",
    "\n",
    "string = change_tag_with_class(html_object, \"div\", \"p\", \"class_s13\", \"class_s13\")\n",
    "\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be62cac-ceda-48a6-8879-58aa2dbf4ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
